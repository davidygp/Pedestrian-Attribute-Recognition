{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/svu/e0384946/anaconda3/lib/python3.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Pedestrian-Attribute-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from batch_engine import valid_trainer, batch_trainer\n",
    "from config import argument_parser\n",
    "from dataset.AttrDataset import AttrDataset, get_transform\n",
    "from loss.CE_loss import CEL_Sigmoid\n",
    "from models.base_block import FeatClassifier, BaseClassifier\n",
    "from models.resnet import resnet50\n",
    "from tools.function import get_model_log_path, get_pedestrian_metrics\n",
    "from tools.utils import time_str, save_ckpt, ReDirectSTD, set_seed\n",
    "\n",
    "import getpass\n",
    "import inspect\n",
    "\n",
    "set_seed(605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(epoch, model, train_loader, valid_loader, criterion, optimizer, lr_scheduler,\n",
    "            path):\n",
    "    maximum = float(-np.inf)\n",
    "    best_epoch = 0\n",
    "\n",
    "    result_list = defaultdict()\n",
    "\n",
    "    for i in range(epoch):\n",
    "\n",
    "        train_loss, train_gt, train_probs = batch_trainer(\n",
    "            epoch=i,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "\n",
    "        valid_loss, valid_gt, valid_probs = valid_trainer(\n",
    "            model=model,\n",
    "            valid_loader=valid_loader,\n",
    "            criterion=criterion,\n",
    "        )\n",
    "\n",
    "        lr_scheduler.step(metrics=valid_loss, epoch=i)\n",
    "\n",
    "        train_result = get_pedestrian_metrics(train_gt, train_probs)\n",
    "        valid_result = get_pedestrian_metrics(valid_gt, valid_probs)\n",
    "\n",
    "        print(f'Evaluation on test set, \\n',\n",
    "              'ma: {:.4f},  pos_recall: {:.4f} , neg_recall: {:.4f} \\n'.format(\n",
    "                  valid_result.ma, np.mean(valid_result.label_pos_recall), np.mean(valid_result.label_neg_recall)),\n",
    "              'Acc: {:.4f}, Prec: {:.4f}, Rec: {:.4f}, F1: {:.4f}'.format(\n",
    "                  valid_result.instance_acc, valid_result.instance_prec, valid_result.instance_recall,\n",
    "                  valid_result.instance_f1))\n",
    "\n",
    "        print(f'{time_str()}')\n",
    "        print('-' * 60)\n",
    "\n",
    "        cur_metric = valid_result.ma\n",
    "\n",
    "        if cur_metric > maximum:\n",
    "            maximum = cur_metric\n",
    "            best_epoch = i\n",
    "            save_ckpt(model, path, i, maximum)\n",
    "\n",
    "        result_list[i] = [train_result, valid_result]\n",
    "\n",
    "    torch.save(result_list, os.path.join(os.path.dirname(path), 'metric_log.pkl'))\n",
    "\n",
    "    return maximum, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_config():\n",
    "    def __init__(self):\n",
    "        self.dataset=\"RAP\"\n",
    "        self.debug='store_false'\n",
    "        self.batchsize=64\n",
    "        self.train_epoch=30\n",
    "        self.height=256\n",
    "        self.width=192\n",
    "        self.lr_ft=0.01\n",
    "        self.lr_new=0.1\n",
    "        self.classifier='base'\n",
    "        self.momentum=0.9\n",
    "        self.weight_decay=5e-4\n",
    "        self.train_split=\"trainval\" #choices=['train' 'trainval']\n",
    "        self.valid_split=\"test\" #choices=['test' 'valid']\n",
    "        self.device=0\n",
    "        self.redirector='store_false'\n",
    "        self.use_bn='store_false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.dataset = \"PA100k\"\n",
    "args.dataset = \"PETA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visenv_name = args.dataset\n",
    "exp_dir = os.path.join('exp_result', args.dataset)\n",
    "model_dir, log_dir = get_model_log_path(exp_dir, visenv_name)\n",
    "\n",
    "# Added for logging purposes\n",
    "user = getpass.getuser() \n",
    "fixed_time_str = time_str()\n",
    "stdout_file = os.path.join(log_dir, \"_\".join(['stdout', user, f'{fixed_time_str}.txt']) )\n",
    "save_model_path = os.path.join(model_dir,  \"_\".join(['ckpt_max', user, f'{fixed_time_str}.pth']) )\n",
    "trackitems_dir = os.path.join(log_dir, \"_\".join(['trackitems', user, f'{fixed_time_str}.txt']) )\n",
    "\n",
    "\n",
    "if args.redirector:\n",
    "    print('redirector stdout')\n",
    "    ReDirectSTD(stdout_file, 'stdout', False)\n",
    "\n",
    "pprint.pprint(OrderedDict(args.__dict__))\n",
    "\n",
    "print('-' * 60)\n",
    "print(f'use GPU{args.device} for training')\n",
    "print(f'train set: {args.dataset} {args.train_split}, test set: {args.valid_split}')\n",
    "\n",
    "train_tsfm, valid_tsfm = get_transform(args)\n",
    "print(train_tsfm)\n",
    "\n",
    "train_set = AttrDataset(args=args, split=args.train_split, transform=train_tsfm)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=args.batchsize,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "valid_set = AttrDataset(args=args, split=args.valid_split, transform=valid_tsfm)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_set,\n",
    "    batch_size=args.batchsize,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f'{args.train_split} set: {len(train_loader.dataset)}, '\n",
    "      f'{args.valid_split} set: {len(valid_loader.dataset)}, '\n",
    "      f'attr_num : {train_set.attr_num}')\n",
    "\n",
    "labels = train_set.label\n",
    "sample_weight = labels.mean(0)\n",
    "\n",
    "backbone = resnet50()\n",
    "classifier = BaseClassifier(nattr=train_set.attr_num)\n",
    "model = FeatClassifier(backbone, classifier)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "criterion = CEL_Sigmoid(sample_weight)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    param_groups = [{'params': model.module.finetune_params(), 'lr': args.lr_ft},\n",
    "                    {'params': model.module.fresh_params(), 'lr': args.lr_new}]\n",
    "else:\n",
    "    param_groups = [{'params': model.finetune_params(), 'lr': args.lr_ft},\n",
    "                    {'params': model.fresh_params(), 'lr': args.lr_new}]\n",
    "optimizer = torch.optim.SGD(param_groups, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=False)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=4)\n",
    "\n",
    "# Added for logging purposes\n",
    "with open(trackitems_dir, \"a\") as f:\n",
    "    code, line_no = inspect.getsourcelines(get_transform)\n",
    "    for line in code:\n",
    "        f.write(str(line))\n",
    "    f.write(str(\"\\n\\n\"))\n",
    "\n",
    "    f.write(str(args.__dict__))\n",
    "    f.write(str(\"\\n\\n\"))\n",
    "\n",
    "    f.write(str(lr_scheduler.__dict__))\n",
    "    f.write(str(\"\\n\\n\"))\n",
    "\n",
    "    model_str = str(model).lower()\n",
    "    have_dropout = 'dropout' in model_str\n",
    "    f.write('dropout: %s' %(have_dropout))\n",
    "    f.write(str(\"\\n\\n\"))\n",
    "\n",
    "    have_leaky_relu = 'leaky_relu' in model_str\n",
    "    f.write('leaky_relu: %s' %(have_leaky_relu))\n",
    "    f.write(str(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric, epoch = trainer(epoch=args.train_epoch,\n",
    "                             model=model,\n",
    "                             train_loader=train_loader,\n",
    "                             valid_loader=valid_loader,\n",
    "                             criterion=criterion,\n",
    "                             optimizer=optimizer,\n",
    "                             lr_scheduler=lr_scheduler,\n",
    "                             path=save_model_path)\n",
    "\n",
    "print(f'{visenv_name},  best_metrc : {best_metric} in epoch{epoch}')\n",
    "\n",
    "# Added for logging purposes\n",
    "with open(trackitems_dir, \"a\") as f:\n",
    "    f.write(f'{visenv_name},  best_metrc : {best_metric} in epoch{epoch}')\n",
    "    f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
